name: Run Amazon Scraper

on:
  schedule:
    - cron: "0 * * * *"  # Runs every 30 minutes
  workflow_dispatch:  # Allows manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:latest
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: 0000
          POSTGRES_DB: capstone_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U myuser"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v3

      - name: ğŸ›  Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: ğŸ“¦ Install Dependencies
        run: |
          pip install -r requirements.txt

      - name: ğŸ›  Set up Database Schema & Load Dummy Data
        run: |
          cd nextjs-fastapi/db/scripts
          python create_table.py
          python dummy_data_for_scraper.py

      - name: ğŸ•· Run Web Scraper
        run: |
          python backend/service/scraper/scrape.py

      - name: ğŸ“¤ Save Scraper Results
        run: |
          mkdir -p output
          mv scraper_output.json output/

      - name: ğŸ“Œ Upload Results as Artifact
        uses: actions/upload-artifact@v3
        with:
          name: scraper-results
          path: output/scraper_output.json
